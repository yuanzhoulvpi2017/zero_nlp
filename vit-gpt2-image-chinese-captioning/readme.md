# ä»‹ç»
## image-encoder-decoder
![](images/vision-encoder-decoder.png)

ä¹‹å‰åœ¨huggingface[https://huggingface.co/nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning)ä¸Šçœ‹åˆ°è¿™ä¸ªæ¨¡å‹.

1. æ„Ÿè§‰è¿™ä¸ªæ¨¡å‹å¾ˆæœ‰è¶£ï¼Œæƒ³æ³•å¾ˆå¥½ã€‚
2. å‘ç°è¿™ä¸ªæ¨¡å‹å…³äºä¸­æ–‡çš„ä¸å¤šã€‚
3. ä¹‹å‰çš„`clip`è®­ç»ƒå…¶å®æŒºå¤±è´¥çš„ï¼Œ`loss`æ²¡æœ‰ä¸‹é™.

ä¸»è¦ä¹Ÿå°±æ˜¯æŠ±ç€å­¦ä¹ çš„æ€åº¦ï¼ŒæŠŠæºç çœ‹æ‡‚ï¼ŒæŠŠæµç¨‹è·‘é€šã€‚åˆ†äº«ä¸­é—´çš„ç»†èŠ‚å’Œè¸©å‘ç»å†ã€‚

## æºç ä»‹ç»

èƒ½æƒ³å‡ºæ¥è¿˜æ˜¯éå¸¸å‰å®³çš„(ç›´å‘¼èƒ¶æ°´æ€ªï¼ï¼)ï¼Œç›®å‰ä»æºç ä¸Šçœ‹ï¼Œå¤§æ¦‚æ˜¯è¿™ä¹ˆä¸€å›äº‹:
![](images/image2.png)
1. ä½¿ç”¨`vit`æ¥ä½œä¸º`encoder`éƒ¨åˆ†ï¼Œè¾“å‡º`encoder_hidden_states`ï¼Œ`ç»¿è‰²éƒ¨åˆ†1`ã€‚
2. ä½¿ç”¨`gpt2`æ¥ä½œä¸º`decoder`éƒ¨åˆ†,æ¥å—`encoder_hidden_states`,`ç»¿è‰²éƒ¨åˆ†3`ã€‚
3. å¦‚æœ`encoder`è¾“å‡ºçš„`encoder_hidden_states`å’Œ`decoder`æ¥å—çš„`encoder_hidden_states`ç»´åº¦ä¸ä¸€æ ·ï¼Œå°±åŠ ä¸ª`linear`,`ç»¿è‰²éƒ¨åˆ†2`ã€‚

## æ¨¡å‹è®­ç»ƒéœ€è¦çš„æ•°æ®æ ·å¼
è®­ç»ƒçš„æ—¶å€™ï¼Œæ¨¡å‹éœ€è¦çš„æ•°æ®ä¸»è¦æœ‰ä¸¤ä¸ªç»´åº¦ï¼š
1. `pixel_value`ï¼š`image`é€šè¿‡`processor`ç”Ÿæˆ
2. `label`ï¼š`text`é€šè¿‡`tokenizer`ç”Ÿæˆçš„`input_ids`ã€‚
3. è®¡ç®—`loss`çš„æ—¶å€™ï¼Œå…¶å®å’Œ`gpt2`ä¸€æ¨¡ä¸€æ ·çš„ï¼ˆè‡ªå›å½’ï¼Œæœ¬è´¨ä¸Šå°±æ˜¯å‘åé”™ä½ä¸€ä¸‹ï¼‰ã€‚


# huggingfaceğŸ¤—

ç›®å‰å·²ç»æŠŠæˆ‘è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œå‘å¸ƒåœ¨`huggingface`ä¸Šäº†ã€‚[https://huggingface.co/yuanzhoulvpi/vit-gpt2-image-chinese-captioning](https://huggingface.co/yuanzhoulvpi/vit-gpt2-image-chinese-captioning)


# è®­ç»ƒ
## æ•°æ®éƒ¨åˆ†
æœ¬æ¨¡å—å¤„ç†æ•°æ®çš„æ–¹å¼å’Œ`clip`æ¨¡å‹å·®ä¸å¤šï¼Œå¯ä»¥çœ‹éš”å£æ–‡ä»¶å¤¹ï¼Œè®­ç»ƒ`clip`çš„æ•°æ®å¤„ç†æ€è·¯ã€‚
### æ³¨æ„âš ï¸
1. åªè¦æŠŠ`processdta_02.ipynb`æ–‡ä»¶æ›¿æ¢å³å¯ã€‚
2. æ‰§è¡Œé¡ºåºä¾ç„¶æŒ‰ç…§ç€`processdta_01.ipynb`ã€`processdta_02.ipynb`ã€`processdta_03.ipynb`ã€‚

## è®­ç»ƒéƒ¨åˆ†`train_encoder_decoder.ipynb`
1. å¤„ç†å›¾åƒï¼Œä½¿ç”¨çš„æ˜¯`"google/vit-base-patch16-224"`æ¨¡å‹ã€‚
2. å¤„ç†æ–‡æœ¬ï¼Œä½¿ç”¨çš„æ˜¯`"yuanzhoulvpi/gpt2_chinese"`æ¨¡å‹ã€‚
3. æœ€åå°±æ˜¯æŠŠä¸¤ä¸ªæ¨¡å‹é€šè¿‡`VisionEncoderDecoderModel`ç²˜èµ·æ¥ã€‚

## è®­ç»ƒçš„loss
![](images/image3.png)

## è®­ç»ƒçš„ä¿¡æ¯
gpuä½¿ç”¨çš„æ˜¯3090ï¼Œæ¨¡å‹å¤§æ¦‚æ˜¯2.16äº¿ä¸ªå‚æ•°ã€‚èŠ±äº†è¶…è¿‡20ä¸ªå°æ—¶ã€‚ä½†æ˜¯å¤§éƒ¨åˆ†æ—¶é—´éƒ½æ˜¯å¡åœ¨IOä¸Šï¼ˆåŠ è½½å›¾ç‰‡ä¸Šï¼‰
![](images/image4.png)


# æ¨ç†
## ç”¨ä½ è‡ªå·±è®­ç»ƒ
å‚è€ƒ`infer_encoder_decoder.ipynb`

## ç›´æ¥ç”¨
```python
from transformers import (VisionEncoderDecoderModel, 
                          AutoTokenizer,ViTImageProcessor)
import torch
from PIL import Image
```
```python
vision_encoder_decoder_model_name_or_path = "yuanzhoulvpi/vit-gpt2-image-chinese-captioning"#"vit-gpt2-image-chinese-captioning/checkpoint-3200"

processor = ViTImageProcessor.from_pretrained(vision_encoder_decoder_model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(vision_encoder_decoder_model_name_or_path)
model = VisionEncoderDecoderModel.from_pretrained(vision_encoder_decoder_model_name_or_path)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

```
```python
max_length = 16
num_beams = 4
gen_kwargs = {"max_length": max_length, "num_beams": num_beams}


def predict_step(image_paths):
    images = []
    for image_path in image_paths:
        i_image = Image.open(image_path)
        if i_image.mode != "RGB":
            i_image = i_image.convert(mode="RGB")

        images.append(i_image)

    pixel_values = processor(images=images, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)

    output_ids = model.generate(pixel_values, **gen_kwargs)

    preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)
    preds = [pred.strip() for pred in preds]
    return preds


predict_step(['bigdata/image_data/train-1000200.jpg'])

```








