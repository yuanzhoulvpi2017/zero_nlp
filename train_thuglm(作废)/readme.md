# ğŸ“£æ³¨æ„è¿™ä¸ªæ–‡ä»¶å¤¹ä½œåºŸï¼Œè¯·æŸ¥çœ‹éš”å£çš„ğŸ“ `simple_thu_chatglm6b`ğŸ“£ğŸ“£




# è®­ç»ƒ`thuglm-6b`æ¨¡å‹

# `thuglm-6b`æ¨¡å‹å’Œ`gpt2`æ¨¡å‹çš„å·®å¼‚

## losséƒ¨åˆ†

1. æŸ¥çœ‹äº†`thuglm-6b`æ¨¡å‹æºç ï¼Œä»–çš„losså’Œ`gpt2`ç­‰è‡ªå›å½’æ¨¡å‹çš„lossï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€æ ·çš„ã€‚(è¿™é‡Œåªæ˜¯è€ƒè™‘è‡ªå›å½’ç±»å‹çš„è®­ç»ƒ)

```python
# 
# è¿™æ˜¯thuglmæ¨¡å‹çš„loss
if labels is not None:
    lm_logits = lm_logits.to(torch.float32)

    # Shift so that tokens < n predict n
    shift_logits = lm_logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

    lm_logits = lm_logits.to(hidden_states.dtype)
    loss = loss.to(hidden_states.dtype)
```

```python
# src/transformers/models/gpt2/modeling_gpt2.py çš„class GPT2LMHeadModel(GPT2PreTrainedModel):
# è¿™æ˜¯gpt2çš„loss 
loss = None
if labels is not None:
    # Shift so that tokens < n predict n
    shift_logits = lm_logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    # Flatten the tokens
    loss_fct = CrossEntropyLoss()
    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))


```

## ä»£ç é£æ ¼

1. `thuglm-6b`æºç å’Œ`transformers`åŒ…çš„`gpt2`æºç ï¼Œé•¿å¾—éå¸¸åƒï¼Œè®¾è®¡æ¨¡å¼æ˜¯ä¸€æ‘¸ä¸€æ ·çš„ã€‚ä»å·¥ç¨‹è§’åº¦æ¥çœ‹ï¼Œä½ åªè¦çœ‹è¿‡`gpt2`
   çš„æºç ï¼Œçœ‹æ‡‚äº†ï¼Œé‚£ä¹ˆ`thuglm-6b`çš„ä»£ç æ¡†æ¶å¯¹ä½ æ¥è¯´è‚¯å®šä¸éš¾ã€‚
2. æ•°å­¦è§’åº¦æ¥è¯´ï¼Œè¿™ä¸ªæˆ‘æ²¡æœ‰çœ‹è¿‡ä¸¤ä¸ªæ¨¡å‹çš„è®ºæ–‡ï¼Œä¸æ•¢èƒ¡è¯´ï¼Œè¿™éƒ¨åˆ†æˆ‘å°±ä¸è§£é‡Šäº†ã€‚

## æ•°æ®è§’åº¦

1. `thuglm-6b`æ¨¡å‹å’Œ`transformers`åŒ…çš„`gpt2`æºç é‡Œé¢çš„æ¨¡å‹ï¼Œåœ¨`forward`æ–¹æ³•é‡Œé¢ï¼Œéœ€è¦çš„å‚æ•°ï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¿æŒä¸€è‡´çš„ï¼Œå› æ­¤ã€‚éœ€è¦çš„æ•°æ®æ ·å¼ï¼Œä¹Ÿéƒ½å·®ä¸å¤šã€‚
2. é‚£ä¹ˆè™½ç„¶ç°åœ¨`thuglm-6b`è¿˜æ²¡æœ‰æ‰€è°“çš„`thuglmForSequenceClassification`ã€`thuglmForTokenClassification`
   ç­‰æ–¹æ³•ï¼Œä½†æ˜¯ç›´æ¥æ¨¡ä»¿`gpt2`çš„é£æ ¼æ¥å†™ï¼Œå°±è¡Œäº†ã€‚å°±æ˜¯`loss`æ›´æ”¹ä¸€ä¸‹ï¼Œä¸‹æ¸¸å±‚æ›´æ”¹ä¸€ä¸‹ã€‚

## æœ¬äººå¯¹`thuglm-6b`æ¨¡å‹çš„æ€åº¦

1. `thuglm-6b`
   æ¨¡å‹ï¼Œæœ€è¿‘å¤ªç«äº†ï¼Œè€Œä¸”åœ¨ä¸­æ–‡è¯­è¨€çš„è¡¨ç°ä¸Šï¼Œæ•ˆæœéå¸¸å¥½[https://github.com/THUDM/ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)
   ï¼Œä½¿ç”¨int8è¿˜å¯ä»¥åœ¨å°æ˜¾å­˜ä¸Šè¿›è¡Œæ¨ç†ï¼Œéå¸¸amazingã€‚
2. ç›®å‰ï¼Œå¾ˆéš¾åœ¨åœ¨å¸‚é¢ä¸Šæ‰¾åˆ°éå¸¸å¥½çš„ä¸­æ–‡`gpt2`æ¨¡å‹ï¼Œå¯èƒ½æ˜¯æ•°æ®æ–¹é¢çš„é—®é¢˜ï¼Œæˆ–è€…æœºå™¨æ–¹é¢çš„é—®é¢˜ã€‚
3. åœ¨æˆ‘çœ¼é‡Œï¼Œæˆ‘å…¶å®å°±æ˜¯æŠŠä»–å½“æˆä¸€ä¸ªåœ¨ä¸­æ–‡é¢†åŸŸè¡¨ç°éå¸¸å¥½çš„`gpt2`æ¨¡å‹è€Œå·²ã€‚ï¼ˆæŠ›å¼€åˆ«çš„æ¡ä»¶ä¸è°ˆï¼‰ã€‚

# è®­ç»ƒ`thuglm-6b`æ¨¡å‹

| åºå·  | ä»‹ç»                                       | æ–‡ä»¶å¤¹                    | æ˜¯å¦å·²å®Œæˆ | æ˜¯å¦è¿˜æœ‰bug |
|-----|------------------------------------------|------------------------|-------|---------|
| 1   | ä½¿ç”¨loraç®—æ³•å¯¹`thuglm-6b`å¾®è°ƒ                   | `v1_train_thuglm-lora` | â˜‘ï¸    | âœ…       |
| 2   | ä½¿ç”¨`transformers`çš„`Trainer`å¯¹`thuglm-6b`å¾®è°ƒ | `v2_train_thuglm`      | â˜‘ï¸    | âœ…       |

## 1. ä½¿ç”¨loraå¾®è°ƒ`thuglm-6b`æ¨¡å‹ æ–‡ä»¶å¤¹ä¸º`v1_train_thuglm-lora`

<details><summary><b>åºå·1</b></summary>
1.ç›®å‰ï¼Œè®­ç»ƒä¸€ä¸ª`thuglm-6b`æ¨¡å‹ï¼Œè¿˜æ˜¯æ¯”è¾ƒè´¹åŠ²çš„ï¼ˆæˆ‘è¿˜æ²¡è¯•è¿‡ï¼Œç›®å‰éƒ½åœ¨ä¼ ä½¿ç”¨loraæ–¹æ³•æ¥è¿›è¡Œè®­ç»ƒï¼‰ã€‚é‚£ä¹Ÿå°±è·Ÿé£å†™ä¸€ä¸ªæ•™ç¨‹ã€‚

2. æ–‡æœ¬ï¼Œå°†ä»‹ç»å¦‚ä½•ä½¿ç”¨`peft`[https://github.com/huggingface/peft](https://github.com/huggingface/peft)
   åŒ…ï¼ˆè¿™ä¸ªåŒ…å®ç°äº†`lora`ç®—æ³•ï¼‰ã€å¯¹`thuglm-6b`è¿›è¡Œå¾®è°ƒã€‚
3. ç¡¬ä»¶è®¾å¤‡æ˜¯3090ï¼ˆæ˜¾å­˜ä¸º24Gï¼‰ã€‚
4. åŒ…æ‹¬æ•°æ®æ•´ç†ã€æ¨¡å‹è½¬æ¢ã€è®­ç»ƒåŠ è½½ç­‰è¯¦ç»†æ­¥éª¤ã€‚

### æ•°æ®éƒ¨åˆ†

åœ¨å‰é¢ä¹Ÿè¯´åˆ°ï¼Œ`thuglm-6b`çš„`ChatGLMForConditionalGeneration`losså’Œ`gpt2`çš„`GPT2LMHeadModel`lossæ˜¯å·®ä¸å¤šçš„ï¼Œéƒ½æ˜¯è‡ªå›å½’æ¨¡å‹ï¼Œå°±æ˜¯åå­—ä¸ä¸€æ ·è€Œå·²ã€‚

å› æ­¤ï¼Œå¯ä»¥çœ‹çœ‹æˆ‘çš„`chinese-gpt2`æ¨¡å‹è®­ç»ƒçš„æ•°æ®è¦æ±‚ã€‚

<details><summary><b>chinese-gpt2æ¨¡å‹æ•°æ®</b></summary>

#### æ•°æ®æ¥æº

1. è·å¾—æ•°æ®:æ•°æ®é“¾æ¥ï¼Œå…³æ³¨å…¬ä¼—å·ã€`ç»Ÿè®¡å­¦äºº`ã€‘ï¼Œç„¶åå›å¤ã€`gpt2`ã€‘å³å¯è·å¾—ã€‚

#### æ•°æ®æ ¼å¼

1. æ•°æ®å…¶å®å°±æ˜¯ä¸€ç³»åˆ—æ–‡ä»¶å¤¹ğŸ“ï¼Œç„¶åæ¯ä¸€ä¸ªæ–‡ä»¶å¤¹é‡Œé¢æœ‰å¤§é‡çš„æ–‡ä»¶ï¼Œæ¯ä¸€ä¸ªæ–‡ä»¶éƒ½æ˜¯`.csv`æ ¼å¼çš„æ–‡ä»¶ã€‚å…¶ä¸­æœ‰ä¸€åˆ—æ•°æ®æ˜¯`content`
2. æ¯ä¸€è¡Œçš„`content`å°±ä»£è¡¨ä¸€å¥è¯,æˆªå›¾å¦‚ä¸‹
   <img src="https://github.com/yuanzhoulvpi2017/zero_nlp/raw/main/images/chinesegpt2_data.png"/>
3. è™½ç„¶æ•°æ®æœ‰15GBé‚£ä¹ˆå¤§ï¼Œä½†æ˜¯å¤„ç†èµ·æ¥ä¸€ç‚¹ä¹Ÿä¸å¤æ‚ï¼Œä½¿ç”¨ `datasets`
   åŒ…ï¼Œå¯ä»¥å¾ˆè½»æ¾çš„å¤„ç†å¤§æ•°æ®ï¼Œè€Œæˆ‘åªéœ€è¦ä¼ é€’æ‰€æœ‰çš„æ–‡ä»¶è·¯å¾„å³å¯ï¼Œè¿™ä¸ªä½¿ç”¨ `glob` åŒ…å°±èƒ½å®Œæˆã€‚

</details>


å½“ç„¶ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç”Ÿæˆä¸€ä¸ªæ•°æ®ï¼Œå¯ä»¥è¿™ä¹ˆå†™

```python
import numpy as np
import pandas as pd
import os

data_dir = "data"
os.makedirs(name=data_dir, exist_ok=True)

for i in range(20):
    data = pd.DataFrame({'sentence': [
                                         'ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäº [General Language Model (GLM)](https://github.com/THUDM/GLM) æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°ã€‚ç»“åˆæ¨¡å‹é‡åŒ–æŠ€æœ¯ï¼Œ'] * 100})
    data.to_csv(f"{data_dir}/{i}.csv", index=False)
```

#### æ•°æ®æ³¨æ„äº‹é¡¹

1. åªè¦æ³¨æ„ï¼Œä½ çš„æ•°æ®é‡Œé¢æ˜¯æœ‰ä¸€åˆ—æ˜¯æ–‡æœ¬ï¼Œè¿™ä¸ªæ–‡æœ¬ä¸éœ€è¦ä»»ä½•æ ‡ç­¾ã€‚æ¯”å¦‚ä¸€åˆ—ä¸º`sentence`ï¼Œæˆ–è€…å«`content`ã€‚è¿™å°±å¯ä»¥äº†ã€‚
2. æˆ‘ä»¬æ•°æ®åŠ è½½ä½¿ç”¨çš„æ˜¯`huggingface`çš„`datasets`åŒ…ï¼Œè™½ç„¶æˆ‘ä»¬è¿™é‡Œä½¿ç”¨çš„æ˜¯`csv`æ–‡ä»¶ï¼Œä½†æ˜¯ï¼Œå®é™…ä¸Šï¼Œä½ ä½¿ç”¨`json`æ ¼å¼çš„æ•°æ®ï¼Œéƒ½æ˜¯å¯ä»¥çš„ã€‚
3.
è®­ç»ƒå¤§æ¨¡å‹ï¼Œéœ€è¦çš„æ•°æ®è‚¯å®šä¹Ÿæ˜¯éå¸¸å¤§ï¼Œæ‹…å¿ƒè‡ªå·±ä¸èƒ½å¤„ç†å‡ ç™¾Gçš„æ•°æ®ä¹ˆï¼Ÿå…¶å®ä¸ç”¨æ‹…å¿ƒï¼Œä½ åªè¦ä¼ é€’æ‰€æœ‰çš„æ•°æ®çš„è·¯å¾„å³å¯ã€‚å‰©ä¸‹çš„ï¼Œå°±å¯ä»¥é `datasets`
æ¥å¸®ä½ è§£å†³ã€‚ä»–ä¼šè‡ªåŠ¨å¯¹æ•°æ®åšå¤„ç†ï¼Œå¹¶ä¸”å¯¹æ•°æ®æ‰€åœ¨çš„ä½ç½®åšå†…å­˜æ˜ å°„ï¼Œå¤„ç†å¤§æ•°æ®ç®€ç›´æ˜¯è½»é£˜é£˜ã€‚

è¿™é‡Œå±•ç¤ºä¸€ä¸‹åŠ è½½æ•°æ®çš„ç»†èŠ‚

```python
from glob import glob
from datasets import load_dataset

all_data_list = glob("v1_train_thuglm_lora/data/*")[:10]  # å¦‚æœæ•°æ®å¤§ï¼ŒæŠŠè¿™ä¸ªåˆ—è¡¨å˜é•¿ä¸€ç‚¹å°±è¡Œäº†ã€‚

dataset = load_dataset(
    "csv",
    data_files={
        "train": all_data_list[:6],
        "validation": all_data_list[6:],
    },
)
```

### æ¨¡å‹è®­ç»ƒ

1. `lora`è¿™ä¸ªç®—æ³•ï¼Œå·²ç»åœ¨`peft`åŒ…ä¸­å®ç°äº†ã€‚
2. æˆ‘çœ‹å¾ˆå¤šäººä¸ºäº†ä½¿ç”¨ä»–ï¼ŒåŒ…è£…äº†å¾ˆå¤šä»£ç ï¼Œå®åœ¨æ˜¯çœ‹ä¸ä¸‹å»äº†ã€‚è¿™é‡Œç»™ä¸€ä¸ªç®€å•çš„ç‰ˆæœ¬ã€‚
3. è¿™ä¸ªç‰ˆæœ¬ï¼Œæ˜¯æ¨¡ä»¿`peft`åŒ…é‡Œé¢çš„`examples`çš„`peft_lora_seq2seq_accelerate_fsdp.py`
   æ–‡ä»¶å†™çš„ã€‚å› æ­¤ï¼Œåœ¨å¤„ç†tokenizerçš„éƒ¨åˆ†ï¼Œå¯èƒ½ä¸å¤ªå¯¹ï¼Œä½†æ˜¯åŸºæœ¬ä¸Šè®­ç»ƒæµç¨‹å·²ç»è·‘é€šäº†ã€‚
4. è™½ç„¶ä¹Ÿæ˜¯è·‘é€šäº†ï¼Œä½†æ˜¯å…·ä½“ç»†èŠ‚ä¸Šï¼Œæˆ‘è¿˜æ˜¯å¯¹`thuglm`
   æ¨¡å‹åšäº†ä¿®æ”¹ï¼Œä¸»è¦æ˜¯ä¸ºäº†è§£å†³`RuntimeError: expected scalar type Half but found Float`é—®é¢˜ã€‚

æœ‰äº›äººå¯èƒ½ä¼šé—®ï¼Œ`lora`ä¹Ÿæ²¡å¯¹`thuglm`è¿™ç±»å‹çš„æ¨¡å‹åšæ”¯æŒå•Šï¼Œä½ è¿™ä¹ˆç”¨ï¼Œéš¾é“ä¸ä¼šæœ‰é—®é¢˜ä¹ˆï¼Ÿ


<details><summary><b>åŸºæœ¬ä¸Šæ˜¯ä¸ä¼šæœ‰é—®é¢˜çš„</b></summary>

1. æŸ¥çœ‹`lora.py`æºç ,åœ¨`target_modules`é‡Œé¢ï¼Œæœ‰åˆ—ä¸¾äº†`['q', 'v']`ã€‚

```python
# src/peft/tuners/lora.py
@dataclass
class LoraConfig(PeftConfig):
    """
    This is the configuration class to store the configuration of a [`~peft.Lora`].

    Args:
        r (`int`): Lora attention dimension
        target_modules (`Union[List[str],str]`): The names of the modules to apply Lora to.
        lora_alpha (`float`): The alpha parameter for Lora scaling.
        lora_dropout (`float`): The dropout probability for Lora layers.
        merge_weights (`bool`):
            Whether to merge the weights of the Lora layers with the base transformer model in `eval` mode.
        fan_in_fan_out (`bool`): Set this to True if the layer to replace stores weight like (fan_in, fan_out)
        enable_lora ( `List[bool]`): Used with `lora.MergedLinear`.
        bias (`str`): Bias type for Lora. Can be 'none', 'all' or 'lora_only'
        modules_to_save (`List[str]`):List of modules apart from LoRA layers to be set as trainable
            and saved in the final checkpoint.
    """

    r: int = field(default=8, metadata={"help": "Lora attention dimension"})
    target_modules: Optional[Union[List[str], str]] = field(
        default=None,
        metadata={
            "help": "List of module names or regex expression of the module names to replace with Lora."
                    "For example, ['q', 'v'] or '.*decoder.*(SelfAttention|EncDecAttention).*(q|v)$' "
        },
    )
```

2. æŸ¥çœ‹`transformers`çš„`T5`æ¨¡å‹æºç ,ä»–é‡Œé¢çš„`['q', 'v']`å¯¹åº”çš„æ˜¯`nn.Linear`å±‚ã€‚

```python
# src/transformers/models/t5/modeling_t5.py
class T5Attention(nn.Module):
    # def __init__(self, config: T5Config, has_relative_attention_bias=False):
    #     super().__init__()
    #     self.is_decoder = config.is_decoder
    #     self.has_relative_attention_bias = has_relative_attention_bias
    #     self.relative_attention_num_buckets = config.relative_attention_num_buckets
    #     self.relative_attention_max_distance = config.relative_attention_max_distance
    #     self.d_model = config.d_model
    #     self.key_value_proj_dim = config.d_kv
    #     self.n_heads = config.num_heads
    #     self.dropout = config.dropout_rate
    #     self.inner_dim = self.n_heads * self.key_value_proj_dim

    self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)
    self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)
    self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)
    self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)
```

3. å› æ­¤ï¼Œæ‰¾åˆ°`thuglm`æ¨¡å‹ä¸­ï¼Œæœ‰å…³`nn.Linear`å±‚çš„åç§°ï¼Œå°±å¯ä»¥äº†ã€‚

4. ä½¿ç”¨`lora`å¯¹`thuglm`æ¨¡å‹åšä¿®æ”¹

```python
from peft import LoraConfig, TaskType, get_peft_model
from peft.utils.other import fsdp_auto_wrap_policy

from train_thuglm.v1_train_thuglm_lora.thuglm.modeling_chatglm import ChatGLMForConditionalGeneration
from train_thuglm.v1_train_thuglm_lora.thuglm.tokenization_chatglm import ChatGLMTokenizer

model = ChatGLMForConditionalGeneration.from_pretrained(
    "THUDM/chatglm-6b", load_in_8bit=False)

tokenizer = ChatGLMTokenizer.from_pretrained("THUDM/chatglm-6b")

# ä½¿ç”¨loraæ¨¡å‹å¯¹thuglmåšè½¬æ¢

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,
    # ['dense','dense_h_to_4h','dense_4h_to_h'] # 'query_key_value',
    target_modules=['dense',
                    'dense_h_to_4h', 'dense_4h_to_h'],
)
model = get_peft_model(model, peft_config)
```

</details>


å…³é”®çš„éƒ¨åˆ†ï¼Œéƒ½å·²ç»è¢«åˆ—ä¸¾å‡ºæ¥äº†ï¼Œå‰©ä¸‹çš„éƒ¨åˆ†ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å’Œè®­ç»ƒ`pytorch`æ¨¡å‹å·®ä¸å¤šäº†ï¼Œå°±ä¸å†ä»‹ç»äº†ã€‚

</details>


## 2. ä½¿ç”¨`transformers`çš„`Trainer`å¯¹`thuglm-6b`å¾®è°ƒ
<details><summary><b>åºå·2</b></summary>

ä¸»è¦åšçš„äº‹æƒ…æœ‰ï¼š
1. ä¿®æ”¹äº†`modeling_chatglm.py`æ¨¡å‹æºç ï¼Œå¯ä»¥ä½¿ç”¨`Tranformers`åŒ…çš„`trainer`æ¥è¿›è¡Œè®­ç»ƒã€‚
2. è‡ªå®šä¹‰æ•°æ®ã€‚


ç¼ºç‚¹
1. éœ€è¦æ‰‹åŠ¨çš„ä»huggingfaceä¸Šä¸‹è½½æ¨¡å‹ä¾èµ–çš„æ–‡ä»¶åˆ°`thu-chatglm-6b`æ–‡ä»¶å¤¹ä¸­ï¼Œä½†æ˜¯è¦ä¿ç•™æˆ‘æ”¾çš„`modeling_chatglm.py`æ–‡ä»¶ã€‚
2. æ˜¾å­˜æ¶ˆè€—å¤§ã€‚3090çš„24Géƒ½é¡¶ä¸ä½ã€‚


</details>


