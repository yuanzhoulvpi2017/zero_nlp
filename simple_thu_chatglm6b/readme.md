# ğŸš€ æœ€ç®€å•ã€æœ€ä¾¿å®œçš„è®­ç»ƒ`thu-chatglm-6b`æ¨¡å‹æ•™ç¨‹ ğŸ¯


# ğŸ“ æ›´æ–°è®°å½•

## **03-27 ç‰ˆæœ¬**
1. ğŸš€**æ·»åŠ äº†å¤šå¡å¹¶è¡Œçš„åŠŸèƒ½**
2. âœ…ä¼šåŸºäºä½ çš„æ˜¾å¡æ•°é‡ï¼Œè‡ªåŠ¨è¿›è¡Œå¹¶è¡Œè®¡ç®—
3. ğŸ˜˜æˆ‘åšçš„äº‹æƒ…ï¼šå°±æ˜¯æ”¹äº†æˆ‘å°±æ˜¯ä¿®æ”¹äº†`thuglm/modeling_chatglm.py`ä»£ç ï¼Œå¯¹é‡Œé¢æ¶‰åŠåˆ°çš„å˜é‡ï¼Œåšäº†è®¾å¤‡çš„æŒ‡å®šï¼ˆè™½ç„¶åŸå§‹çš„ä»£ç ä¹Ÿåšäº†ï¼Œä½†æ˜¯åšäº†å¹¶ä¸å……åˆ†ï¼‰
4. ğŸ¤—æœ¬è´¨ä¸Šï¼Œä½¿ç”¨çš„å°±æ˜¯pytorchçš„`nn.DataParallel`åŠŸèƒ½,å› ä¸ºæˆ‘å°±æ˜¯æƒ³è®©ä»–æ”¯æŒ`transformers`çš„`Trainer`ã€‚

### â›”ï¸æ³¨æ„äº‹é¡¹
1. åœ¨ä½¿ç”¨çš„æ—¶å€™ï¼Œç¬¬ä¸€å¼ å¡çš„å‹åŠ›è¦å¤§ä¸€ç‚¹ã€‚
2. æˆ‘åœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œå‘ç°åœ¨3ä¸ª3090ä¸Šï¼Œæ˜¯å®Œå…¨æ²¡æœ‰é—®é¢˜çš„ã€‚ä½†æ˜¯åœ¨4ä¸ª3090çš„æ—¶å€™ï¼Œä¼šå‡ºç°å°bugï¼š`RuntimeError: CUDA error: an illegal memory access was encountered`ï¼ˆè¯´æ˜æˆ‘çš„deivceåˆ†é…ä¾ç„¶ä¸å¤ªå¯¹ï¼‰ã€‚
3. æˆ‘åœ¨ä¸¤ä¸ªT4çš„æœºå™¨ä¸Šè®­ç»ƒï¼Œä¼šå‡ºç°ä¸€ä¸ªå°bug:`TypeError: 'NoneType' object is not subscriptable`ï¼ˆè¿™ä¸ªåº”è¯¥æ˜¯æˆ‘çš„ä»£ç ä¸å¯¹ï¼‰
4. è™½ç„¶bugä¸å°‘ï¼Œä½†æ˜¯å¯ä»¥çŸ¥é“åœ¨ä»€ä¹ˆåœ°æ–¹ä¼˜åŒ–ï¼ŒçŸ¥é“æ”¹å“ªé‡Œäº†ï¼Œåé¢å°†ç»§ç»­ä¼˜åŒ–ï¼ï¼ï¼ğŸ¯ å†²ï¼ï¼ï¼

## **03-24 ç‰ˆæœ¬**
1. ğŸ’» ç°åœ¨å¯ä»¥åœ¨16Gæ˜¾å­˜çš„æ˜¾å¡ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆåœ¨`batchsize=1,content_length=512`çš„æƒ…å†µä¸‹ï¼‰
2. ğŸš€ä½¿ç”¨äº†`torch.utils.checkpoint`ï¼Œé™ä½äº†æ˜¾å­˜çš„å ç”¨ï¼ˆä»ä¹‹å‰çš„24Gé™ä½åˆ°15.2Gå·¦å³ï¼‰ï¼Œä½†æ˜¯è®­ç»ƒçš„æ—¶é—´èŠ±è´¹æ›´å¤šã€‚ï¼ˆå¦‚æœä½ æƒ³å…³é—­è¿™ä¸ªåŠŸèƒ½ï¼Œåœ¨`thuglm/modeling_chatglm.py`æ–‡ä»¶çš„ç¬¬`713`è¡Œ`self.gradient_checkpointing = True`ä¸­ï¼ŒæŠŠ`True`æ”¹ä¸º`False`å³å¯ï¼‰
3. ğŸ¤– ç²¾åº¦ä¾ç„¶æ˜¯ä½¿ç”¨çš„`fp16`ï¼Œè€Œä¸æ˜¯`int8`.
4. ğŸ’¨ ä¾ç„¶ä½¿ç”¨äº†`lora`æ–¹æ³•ï¼Œå¦‚æœä¸æƒ³ä½¿ç”¨è¿™ä¸ªæ–¹æ³•ï¼Œæˆ‘åç»­å¯ä»¥æŠŠè¿™ä¸ªæ–¹æ³•å…³é—­ã€‚
5. ğŸ“£ ç°åœ¨ä½ å¯ä»¥æŠŠ`content_length`è°ƒæ•´åˆ°`1024`ï¼Œ `batchsize`å¯ä»¥è°ƒæ•´åˆ°`4`ï¼Œå³ä½¿è¿™æ ·ï¼Œæ˜¾å­˜ä¾ç„¶ç»´æŒåœ¨23Gå·¦å³ã€‚
![](images/WechatIMG15931.jpeg)

## **03-22 ç‰ˆæœ¬**
1. ğŸ’»ä¸€ä¸ª3090æ¶ˆè´¹çº§çš„æ˜¾å¡å°±å¯ä»¥è®­ç»ƒ
2. ğŸ¯æ”¯æŒ`tensorboard`ç­‰å„ç§èŠ±é‡Œèƒ¡å“¨å°æ’ä»¶
3. ğŸš€ä¹Ÿå¯ä»¥å¤šå¡å¹¶è¡Œï¼Œè®­ç»ƒéå¸¸å¿«
4. âœ…æ•°æ®åªéœ€è¦æ–‡æœ¬å³å¯ï¼Œä¸ç®¡æ˜¯jsonè¿˜æ˜¯csvæ–‡ä»¶ï¼Œéƒ½å¯ä»¥ï¼Œæ— ç›‘ç£å­¦ä¹ ï¼Œæ•´ç†æ•°æ®æ›´è½»æ¾
5. ğŸ“è®­ç»ƒä»£ç æ¯”ä»¥å¾€çš„æ•™ç¨‹æ›´åŠ ç®€å•ï¼Œå¯ä»¥è¯´æ˜¯æœ€ç®€å•çš„è®­ç»ƒ`thu-chatglm-6b`æ•™ç¨‹äº†


## æˆ‘åšäº†ä»€ä¹ˆï¼Œæœ‰ä»€ä¹ˆæ•ˆæœ
åªæ˜¯å¯¹`transofrmers`åŒ…çš„`Trainer`ç±»åšäº†ä¿®æ”¹ï¼Œå¯¹`modeling_chatglm.py`ä»£ç ä¹Ÿåšäº†ä¿®æ”¹ã€‚
è¿™ä¹ˆåšï¼Œå¯ä»¥è®©ä½ åœ¨æ‹¥æœ‰22Gæ˜¾å­˜çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥è®­ç»ƒ`thu-chatglm-6b`æ¨¡å‹ã€‚

é‚£ä¹ˆï¼ŒåŸºäº`Trainer`çš„ä¸°å¯Œæ–¹æ³•ï¼Œä½ å¯ä»¥åšå¾ˆå¤šäº‹æƒ…ã€‚è€Œä¸”ä½¿ç”¨`peft`åŒ…[https://github.com/huggingface/peft](https://github.com/huggingface/peft)çš„`lora`ç®—æ³•ï¼Œè®©ä½ åœ¨ä¸€ä¸ªæ¶ˆè´¹çº§åˆ«çš„æ˜¾å¡ä¸Šï¼Œå°±å¯ä»¥è®­ç»ƒ`thu-chatglm-6b`æ¨¡å‹ã€‚

# æ•™ç¨‹

## æ¨¡å‹éƒ¨åˆ†

ä¸ºäº†æœ‰æ¡ç†æ€§ï¼Œæˆ‘æŠŠè¿™ä¸ªæ¨¡å‹çš„æ‰€æœ‰ä»£ç å…¨éƒ¨éƒ½æ”¾åœ¨ğŸ“`thuglm`æ–‡ä»¶å¤¹ä¸‹ã€‚
![](images/æˆªå±2023-03-22%2019.08.54.png)


ä½†æ˜¯ï¼Œä½ åœ¨ä»githubä¸Šä¸‹è½½æˆ‘è¿™ä¸ªä»“åº“åï¼Œæ˜¯çœ‹ä¸åˆ°è¿™å‡ ä¸ªæ–‡ä»¶çš„ï¼š
1. `pytorch_model-00001-of-00008.bin`ã€
2. `pytorch_model-00002-of-00008.bin`ã€
3. `pytorch_model-00002-of-00008.bin`ã€
4. `pytorch_model-00003-of-00008.bin`ã€
5. `pytorch_model-00004-of-00008.bin`ã€
6. `pytorch_model-00005-of-00008.bin`ã€
7. `pytorch_model-00006-of-00008.bin`ã€
8. `pytorch_model-00007-of-00008.bin`ã€
9. `pytorch_model-00008-of-00008.bin`ã€
10. `ice_text.model`

ä½ éœ€è¦ä»[https://huggingface.co/THUDM/chatglm-6b/tree/main](https://huggingface.co/THUDM/chatglm-6b/tree/main) è¿™é‡ŒæŠŠä¸Šé¢åˆ—ä¸¾çš„æ–‡ä»¶ä¸‹è½½ä¸‹æ¥ã€‚

æ³¨æ„æŸ¥çœ‹ï¼Œåœ¨è¿™ä¸ªé“¾æ¥é‡Œé¢ï¼Œæ¯ä¸ªæ–‡ä»¶åé¢éƒ½æœ‰ä¸€ä¸ªä¸‹è½½çš„ç®­å¤´
![](images/æˆªå±2023-03-22%2019.06.22.png)


**ä¸‹è½½åï¼ŒæŠŠä¸‹è½½çš„æ–‡ä»¶éƒ½æ”¾åœ¨`thuglm`æ–‡ä»¶å¤¹ä¸‹é¢ï¼Œç„¶åå’Œæˆ‘çš„æˆªå›¾æ¯”å¯¹ä¸€ä¸‹ï¼Œæ˜¯ä¸æ˜¯æœ‰ä»€ä¹ˆå‡ºå…¥ã€‚**

åˆ°è¿™é‡Œï¼Œæ¨¡å‹éƒ¨åˆ†å°±è§£å†³äº†ã€‚
## æ•°æ®éƒ¨åˆ†

æˆ‘è¿™é‡Œç»™ä¸€ä¸ªæ ·æœ¬æ•°æ®ï¼Œå°±æ˜¯å•çº¯å‚è€ƒï¼š

**é“¾æ¥ï¼šhttps://pan.baidu.com/s/1HZoEofUmXgq68-1sqZNVTw?pwd=1u20 
æå–ç ï¼š1u20**

é‡Œé¢æœ‰ä¸€ä¸ªåå«`data2.zip`çš„å‹ç¼©åŒ…æ–‡ä»¶ï¼Œç›´æ¥è§£å‹åˆ°å½“å‰æ–‡ä»¶å¤¹å°±è¡Œäº†ã€‚

`data2`å±•å¼€æ˜¯è¿™æ ·çš„ï¼š

![](images/æˆªå±2023-03-22%2019.17.13.png)

`data2`åœ¨æ•´ä¸ªæ–‡ä»¶ç³»ç»Ÿä¸Šæ¥çœ‹ï¼Œæ˜¯è¿™æ ·çš„ï¼š

![](images/æˆªå±2023-03-22%2019.18.07.png)


### æ•°æ®è¯¦è§£
1. æ³¨æ„åˆ°æ•°æ®é‡Œé¢æ˜¯æœ‰ä¸€åˆ—ï¼Œå«`content`
2. ä½ æƒ³æ¢æˆåˆ«çš„æ•°æ®éƒ½æ˜¯å¯ä»¥çš„ï¼Œæœ¬è´¨ä¸Šæ˜¯ä½¿ç”¨çš„`datasets`è¿™ä¸ªåŒ…ï¼Œä¹Ÿæ˜¯`huggingface`å‡ºå“çš„ã€‚


# å®‰è£…

ä¸Šé¢æ˜¯æ–‡ä»¶å·¥ç¨‹ï¼Œè¿™é‡Œå¼€å§‹è¯´å®‰è£…åŒ…ï¼Œç›´æ¥ä½¿ç”¨`pip`å®‰è£…

```bash
pip install protobuf==3.20.0 transformers icetk cpm_kernels peft
```

å°±è¿™ä¹ˆç®€å•ï¼Œä¸éœ€è¦å®‰è£…åˆ«çš„ä¸œè¥¿äº†

# âœ… è®­ç»ƒéƒ¨åˆ†
è®­ç»ƒéƒ¨åˆ†ï¼Œç›´æ¥è¿è¡Œ`train_chatglm6b.py`ä»£ç ï¼Œå°±å¯ä»¥äº†ï¼Œä½†æ˜¯è¿™é‡Œï¼Œç›´æ¥åœ¨å†™ä¸€æ¬¡è¯¦ç»†çš„è®²è§£ã€‚

## åŠ è½½åŒ…
```python

#è¿™ä¸ªæ˜¯æˆ‘ä»transformersé‡Œé¢å¤åˆ¶çš„Trainerï¼Œä¸ºchatglmåšäº†é€‚åº”
from MyTrainer import Trainer


from transformers import TrainingArguments
from transformers import DataCollatorForLanguageModeling
import random
from glob import glob
from datasets import load_dataset, DatasetDict # åŠ è½½æ•°æ®ç”¨çš„
from transformers import AutoTokenizer, AutoModel

# loraå·²ç»åœ¨pefté‡Œé¢å®ç°äº†ï¼Œå› æ­¤ä½¿ç”¨peftåŒ…å³å¯
from peft import get_peft_model, LoraConfig, TaskType

```

## åŠ è½½æ¨¡å‹

å› ä¸ºæˆ‘ä»¬å·²ç»ä»`huggingface-hub`ä¸ŠæŠŠè¿™ä¸ªæ¨¡å‹éœ€è¦çš„ä¸œè¥¿å…¨éƒ¨ä¸‹è½½åˆ°`thuglm`æ–‡ä»¶é‡Œé¢äº†ï¼Œæ‰€ä»¥è¿™é‡Œå¯¼å…¥æ¨¡å‹ï¼Œåªéœ€è¦ä½¿ç”¨`"thuglm"`è·¯å¾„å°±è¡Œäº†
```python

tokenizer = AutoTokenizer.from_pretrained("thuglm", trust_remote_code=True)
model = AutoModel.from_pretrained(
    "thuglm", trust_remote_code=True).half().cuda()

```

## åˆ©ç”¨`lora`å¯¹æ¨¡å‹åšè½¬æ¢

1. ç®€å•æ¥è¯´ï¼Œ`lora`å°±æ˜¯å¯¹`nn.Linear`åšå¤„ç†ï¼Œè€Œæ¨¡å‹é‡Œé¢æœ‰`nn.Linear`å±‚çš„åå­—ä¸»è¦ä¸º`'dense','dense_h_to_4h','dense_4h_to_h','query_key_value',`
2. ä½†æ˜¯æˆ‘ä»¬è¿™é‡Œåªæ˜¯å¯¹`query_kery_value`åšå¤„ç†ï¼Œä½ å–œæ¬¢ï¼Œæ¢æˆåˆ«çš„åº”è¯¥ä¹Ÿå¯ä»¥ã€‚
3. åæ­£åˆ°è¿™é‡Œï¼Œæ¨¡å‹å·²ç»è¢«`peft`åŒ…ç»™åŒ…è£…å¥½äº†ã€‚
```python

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1,
    # ['dense','dense_h_to_4h','dense_4h_to_h'] # 'query_key_value',
    target_modules=['query_key_value', ],
)
model = get_peft_model(model, peft_config)
```

## åŠ è½½æ•°æ®
1. æˆ‘ä»¬ä¼ é€’æ•°æ®çš„æ—¶å€™ï¼Œæ˜¯é€šè¿‡æ•°æ®çš„è·¯å¾„æ¥ä¼ é€’çš„ã€‚
2. å› æ­¤åœ¨`all_file_list`è¿™ä¸ªåˆ—è¡¨é‡Œé¢ï¼Œå‚¨å­˜çš„éƒ½æ˜¯æ•°æ®çš„è·¯å¾„ã€‚
3. æˆ‘ä¹Ÿå°±æ˜¯éšä¾¿åˆ†äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä½ æŒ‰ä½ éœ€æ±‚æ¥ã€‚
4. æ³¨æ„ï¼Œæ•°æ®é‡Œé¢ï¼Œæ¯ä¸€ä¸ª`csv`æ–‡ä»¶éƒ½æ˜¯æœ‰ä¸€åˆ—å«`content`åˆ—ã€‚è¿™ä¸ªå’Œä¸‹æ–‡å‘¼åº”ã€‚
```python
random.seed(42)

all_file_list = glob(pathname="data2/*")
test_file_list = random.sample(all_file_list, 50)
train_file_list = [i for i in all_file_list if i not in test_file_list]
# len(train_file_list), len(test_file_list)

raw_datasets = load_dataset("csv", data_files={
                            'train': train_file_list, 'valid': test_file_list}, cache_dir="cache_data")

```

## æ•°æ®è½¬æ¢

1. `context_length`è¡¨ç¤ºæ¯ä¸€æ¡æ–‡æœ¬çš„é•¿åº¦ï¼Œè¿™é‡Œè®¾ç½®çš„ä¸ºæœ€é«˜512.
2. æ³¨æ„`tokenize`å‡½æ•°é‡Œé¢ï¼Œæœ‰ä¸€ä¸ª`element['content']`ï¼Œè¿™å¥è¯å°±æ˜¯è¦æŠŠæ•°æ®çš„è¿™ä¸€åˆ—ï¼Œé€šè¿‡`tokenizer`ç»™è½¬æ¢æˆ`input_ids`å­—å…¸ã€‚
3. ä½ æ³¨æ„åˆ°æœ€åä¸€ä¸ª`data_collactor`äº†ä¹ˆï¼Œä»–åœ¨è®­ç»ƒçš„æ—¶å€™ï¼Œä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å˜é‡å«`label`ï¼Œè€Œè¿™ä¸ª`label`æœ¬è´¨ä¸Šå°±æ˜¯`input_ids`ã€‚è‡ªå›å½’æ¨¡å‹éƒ½æ˜¯è¿™ä¹ˆç©çš„ã€‚ï¼ˆè™½ç„¶çœ‹ç€éƒ½æ˜¯ä½¿ç”¨ä¸€åˆ—æ•°æ®ï¼Œæ²¡æœ‰æ ‡ç­¾ï¼Œä½†æ˜¯åœ¨è®¡ç®—`loss`çš„æ—¶å€™ï¼Œå°±æ˜¯é”™ä½äº†ä¸€ä¸‹ï¼‰
   
```python

context_length = 512 # è¿™ä¸ªå¤§å°ï¼ŒåŸºæœ¬ä¸å½±å“æ˜¾å­˜ï¼Œå› æ­¤è®¾ç½®ä¸º1024ä¹Ÿè¡Œï¼Œç›®å‰ä¸çŸ¥é“chatglmè¦æ±‚çš„æ–‡æœ¬é•¿åº¦ä¸Šé™ä¸ºå¤šå°‘

def tokenize(element):
    outputs = tokenizer(
        element["content"],
        truncation=True,
        max_length=context_length,
        return_overflowing_tokens=True,
        return_length=True,
    )
    input_batch = []
    for length, input_ids in zip(outputs["length"], outputs["input_ids"]):
        if length == context_length:
            input_batch.append(input_ids)
    return {"input_ids": input_batch}


tokenized_datasets = raw_datasets.map(
    tokenize, batched=True, remove_columns=raw_datasets["train"].column_names
)
tokenized_datasets

data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

```


## è®­ç»ƒå‚æ•°è®¾ç½®å’Œè®­ç»ƒ
1. `output_dir="test003"`è¿™ä¸ªè¡¨ç¤ºè¦æŠŠæ¨¡å‹ä¿å­˜åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œæ³¨æ„è¿™é‡Œï¼Œä¸‹é¢è¦å‘¼åº”ä¸Šã€‚
2. `per_device_train_batch_size=1`è¡¨ç¤ºçš„æ˜¯è®­ç»ƒæ•°æ®çš„`batch_size=1`,`per_device_eval_batch_size=1`è¡¨ç¤ºéªŒè¯æ•°æ®çš„`batch_size=1`ï¼Œç®€ç›´æ˜¯åˆ€å‰‘èˆ”è¡€ï¼Œåˆ°è¿™é‡Œï¼Œæ˜¾å­˜åŸºæœ¬ä¸Šæ˜¯åˆšåˆšå¥½ï¼Œè¿˜æœ‰200å¤šmbçš„æ˜¾å­˜ï¼Œå°±è¦çˆ†ç‚¸äº†.
3. `eval_steps`ã€`logging_steps`ã€`save_steps`è¿™ä¸‰ä¸ªå€¼éƒ½æ˜¯ä¸€æ ·çš„ï¼Œè¡¨ç¤ºæ¯éš”100ä¸ª`batch_size`å°±å¯¹æ¨¡å‹è¿›è¡Œè¯„ä¼°ï¼Œæ‰“å°æˆç»©ï¼Œä¿å­˜æ¨¡å‹ï¼Œå¦‚æœä½ æ•°æ®ä¸å¤šï¼Œå¯ä»¥æŠŠè¿™ä¸ª100è°ƒæ•´ä¸ºåˆé€‚çš„å¤§å°ã€‚
4. ç„¶åè®­ç»ƒéƒ¨åˆ†å°±ç»“æŸäº†ã€‚
```python
args = TrainingArguments(
    output_dir="test003",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    evaluation_strategy="steps",
    eval_steps=100,
    logging_steps=100,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    weight_decay=0.1,
    warmup_steps=1_000,
    lr_scheduler_type="cosine",
    learning_rate=5e-4,
    save_steps=100,
    fp16=True,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    tokenizer=tokenizer,
    args=args,
    data_collator=data_collator,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["valid"],
)
trainer.train()
```


# âœ… æ¨ç†éƒ¨åˆ†
1. æ¨ç†éƒ¨åˆ†ï¼Œç›´æ¥çœ‹`infer.ipynb`ä»£ç 
2. èƒ½åˆ°è¿™é‡Œï¼Œä¹Ÿæ˜¯æ­å–œä½ ï¼Œå¾®è°ƒæ¨¡å‹å·²ç»æˆåŠŸäº†ã€‚è¿™ä¸ªæ—¶å€™ï¼Œåœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œè‚¯å®šæœ‰ä¸€ä¸ªæ–‡ä»¶å¤¹å«`test003`ï¼ˆå°±æ˜¯ä¸Šé¢`output_dir="test003"`å¯¹åº”çš„æ–‡ä»¶å¤¹ï¼‰
3. åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œä½ è‚¯å®šå¯ä»¥çœ‹åˆ°å¾ˆå¤š`checkpoint-xxx`ï¼Œé€‰æ‹©ä¸€ä¸ªä½ å–œæ¬¢çš„ï¼ˆå½“ç„¶ï¼Œè‚¯å®šæ˜¯æœ€å¥½é€‰æ‹©æœ€æ–°çš„ï¼‰ã€‚
4. ** ç„¶åæŠŠ`thuglm/config.json`æ–‡ä»¶å¤åˆ¶åˆ°`test003/checkpoint-xxx`é‡Œé¢ã€‚** è¿™ä¸ªæ­¥éª¤éå¸¸é‡è¦ã€‚


## åŠ è½½åŒ…

```python
from transformers import AutoTokenizer
from thuglm.modeling_chatglm import ChatGLMForConditionalGeneration
import torch
```


## åŠ è½½æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹

```python

# è¿™ä¸ªæ˜¯æˆ‘ä»¬è®­ç»ƒå¥½çš„æ¨¡å‹
model = ChatGLMForConditionalGeneration.from_pretrained("test003//checkpoint-200").cuda() #

# è¿™ä¸ªæ˜¯åŸå§‹å‘å¸ƒçš„æ¨¡å‹
# model = ChatGLMForConditionalGeneration.from_pretrained("thuglm").half().cuda() #
```

## åŠ è½½tokenizer
1. å› ä¸ºæˆ‘ä»¬æ¨¡å‹åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œæ²¡æœ‰ä¿å­˜tokenizerï¼Œè€Œä¸”åœ¨è®­ç»ƒçš„è¿‡ç¨‹ä¸­ï¼Œä¹Ÿæ²¡ä»€ä¹ˆæ–°çš„wordã€‚æ‰€ä»¥ç›´æ¥ä½¿ç”¨åŸå§‹çš„tokenizer
```python
tokenizer = AutoTokenizer.from_pretrained("thuglm", trust_remote_code=True)
```

## æ¨ç†ï¼Œç”Ÿæˆæ–‡æœ¬

```python
with torch.autocast("cuda"):
    res, history = model.chat(tokenizer=tokenizer, query="ä½ æ˜¯è°? æˆ‘æ˜¯ç”±è‰¯ç¦è·¯ç¨‹åºå‘˜è®­ç»ƒçš„ä¸€ä¸ªAIæ¨¡å‹")
        # res = model.forward(input_ids=all_input.get('input_ids').cuda())
    print(res)
```


# ğŸ¯
1. ä½ åªéœ€è¦æ‹¥æœ‰ä¸€ä¸ª3090å³å¯ï¼ˆåªè¦æ˜¾å­˜æœ‰24Gå°±è¡Œäº†ï¼‰
2. ç›®å‰è¿˜æ²¡æœ‰å°è¯•è¿‡å¤šå¡ï¼Œä¸‹æ¬¡å»å…¬å¸è¯•ä¸€è¯•

